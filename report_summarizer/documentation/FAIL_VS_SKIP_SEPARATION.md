# FAIL vs. SKIP Separation Implementation

## ğŸ¯ Goal

Separate FAILED tests from SKIPPED tests throughout the entire application to help identify:
- **FAILED tests** = Code/logic issues that need fixing
- **SKIPPED tests** = Environment/infrastructure issues (device unavailable, cloud problems)

## âœ… Changes Made

### 1. **AI Prompt Updates** (`circuit_langchain_summarizer.py`)

#### Full Analysis Prompt
**Before:** Analyzed all "failures" together
```python
"Please analyze this test report and provide a comprehensive summary 
focusing on the failures."
```

**After:** Separate sections for FAILED and SKIPPED
```python
"Provide analysis with these DISTINCT sections:

1. OVERALL TEST SUMMARY
   - Total, passed, failed, skipped counts
   
2. FAILED TESTS ANALYSIS (Code/Logic Issues)
   - List each FAILED test
   - Root cause (code bugs, assertion failures)
   - Recommendations for code fixes

3. SKIPPED TESTS ANALYSIS (Environment/Infrastructure Issues)
   - List each SKIPPED test
   - Reason for skip (device state, cloud issues)
   - Patterns (e.g., 'all tests on device X')
   - Recommendations for environment fixes
"
```

**Benefits:**
- âœ… AI clearly separates code issues from environment issues
- âœ… Different recommendations for FAILED vs. SKIPPED
- âœ… Easier to identify if problem is device-specific or cloud-wide

---

#### Quick Analysis Prompt
**Before:** Mixed failures together
```python
"Focus on:
- Overall test statistics
- List of failed tests
- Brief description of each failure"
```

**After:** Separate FAILED and SKIPPED
```python
"Provide a brief summary with two sections:

1. FAILED TESTS (Code Issues)
   - Count and list of failed tests
   - Brief description of each failure

2. SKIPPED TESTS (Environment Issues)
   - Count and list of skipped tests
   - Brief reason for each skip
   - Patterns (e.g., all on same device/cloud)
"
```

**Benefits:**
- âœ… Quick triage: immediately see if it's code or environment
- âœ… Fast identification of device/cloud problems

---

#### Comparison Prompt
**Before:** Compared "failures" generically
```python
"2. COMMON FAILURES
   - Tests that failed in BOTH builds
   
3. REGRESSIONS (New Failures)
   - Tests that passed in Build 1 but failed in Build 2"
```

**After:** Separate FAILED and SKIPPED comparisons
```python
"2. FAILED TESTS COMPARISON (Code Issues)
   - Common failures in BOTH builds (persistent code issues)
   - New failures in Build 2 (code regressions)
   - Resolved failures in Build 2 (code fixes)

3. SKIPPED TESTS COMPARISON (Environment Issues)
   - Common skips in BOTH builds (persistent environment issues)
   - New skips in Build 2 (environment degradation)
   - Resolved skips in Build 2 (environment fixes)
   - Patterns (specific devices, cloud dependencies)

5. ENVIRONMENT vs. CODE HEALTH
   - Is quality declining due to code or environment?
"
```

**Benefits:**
- âœ… Track code quality separately from environment health
- âœ… See if environment is degrading over time
- âœ… Identify device-specific problems across builds

---

### 2. **UI Metrics Updates** (`report_analyzer_web.py`)

#### Before: 4 Columns (Total, Passed, Failed, Mode)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Total   â”‚  Passed  â”‚  Failed  â”‚   Mode   â”‚
â”‚   174    â”‚   161    â”‚    13    â”‚   FULL   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
- FAIL and SKIP mixed together
- Can't distinguish code from environment issues

#### After: 5 Columns (Total, Passed, Failed, Skipped, Mode)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Total â”‚ âœ… Passed â”‚ âŒ Failed  â”‚ â­ï¸ Skipped â”‚ Mode â”‚
â”‚  174  â”‚ 161 (95%) â”‚ 10 (5.9%)  â”‚ 3 (1.7%)   â”‚ FULL â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
```

**Key Changes:**
- Added 5th column for SKIPPED tests
- Emoji indicators: âœ… Passed, âŒ Failed, â­ï¸ Skipped
- Separate percentages for each category
- Pass rate calculated excluding skipped (passed/testable)

**Calculation Logic:**
```python
total_tests = 174
failed_tests = 10
skipped_tests = 3
passed_tests = 161  # total - failed - skipped

testable_count = 171  # total - skipped
pass_rate = (161 / 171) * 100 = 94.2%
fail_rate = (10 / 171) * 100 = 5.8%
skip_rate = (3 / 174) * 100 = 1.7%
```

**Benefits:**
- âœ… Immediately see if tests are failing or being skipped
- âœ… Pass rate excludes skipped tests (more accurate)
- âœ… Skip rate helps identify environment stability

---

### 3. **Comparison Table Updates** (`report_analyzer_web.py`)

#### Before: 4 Columns
```
| Build                     | Total | Passed | Failed | Pass Rate |
|---------------------------|-------|--------|--------|-----------|
| âœ… qatest_build_7499     | 174   | 161    | 13     | 92.5%     |
| âœ… staging_build_7500    | 174   | 170    | 4      | 97.7%     |
```

#### After: 5 Columns with Percentages
```
| Build                     | Total | âœ… Passed     | âŒ Failed     | â­ï¸ Skipped    |
|---------------------------|-------|---------------|---------------|---------------|
| âœ… qatest_build_7499     | 174   | 161 (94.2%)   | 10 (5.8%)     | 3 (1.7%)      |
| âœ… staging_build_7500    | 174   | 170 (99.4%)   | 1 (0.6%)      | 3 (1.7%)      |
```

**Benefits:**
- âœ… Compare FAIL vs. SKIP across builds
- âœ… See if environment issues are consistent or build-specific
- âœ… Identify if one build has more skips (environment degradation)

**Analysis Example:**
```
Both builds have 3 skips (1.7%) - Same environment issues
qatest has 10 fails vs. staging has 1 fail - Code is worse in qatest
Conclusion: Fix code in qatest, investigate 3 common environment issues
```

---

### 4. **History Display Updates** (`utils/session_manager.py`)

#### Before
```
**staging_sars_sanity_build_7500**
ğŸ“… 2026-01-21 22:30:00 | ğŸ” FULL
âœ… 161/174 passed (92.5%)
```

#### After
```
**staging_sars_sanity_build_7500**
ğŸ“… 2026-01-21 22:30:00 | ğŸ” FULL
âœ… 161/171 passed (94.2%)
â­ï¸ 3 skipped
```

**Changes:**
- Pass rate calculated from testable tests (excluding skipped)
- Skipped count displayed separately (only if > 0)
- More accurate representation of test health

---

## ğŸ“Š Use Cases

### Use Case 1: Device-Specific Issues

**Scenario:** All tests on a particular device are being skipped

**Before (FAIL and SKIP mixed):**
```
13 failures detected
Failed tests: (mix of real failures and skipped tests)
```
âŒ Hard to identify pattern

**After (FAIL and SKIP separate):**
```
FAILED TESTS: 10
- test_config_validation
- test_policy_update
- ...

SKIPPED TESTS: 3
- c220m5_all_tests (device unavailable)
- c220m5_inventory (device unavailable)
- c220m5_config (device unavailable)

Pattern: All skips are on c220m5 device
Action: Check c220m5 device health, not code issues
```
âœ… Clear device problem identified

---

### Use Case 2: Cloud Infrastructure Problems

**Scenario:** Cloud API is down, causing test skips

**Before:**
```
20 failures
Failures across multiple tests
```
âŒ Looks like massive code regression

**After:**
```
FAILED TESTS: 5
- test_local_validation
- test_profile_schema
- ...

SKIPPED TESTS: 15
- All cloud-dependent tests skipped
- Reason: Cloud API unreachable

Pattern: All skips are cloud-dependent
Action: Wait for cloud recovery, not a code issue
```
âœ… Cloud issue identified, not code regression

---

### Use Case 3: Environment Degradation Over Time

**Scenario:** Compare builds to see if environment is getting worse

**Before:**
```
Build 7490: 15 failures
Build 7500: 18 failures
Trend: Declining (more failures)
```
âŒ Looks like code quality declining

**After:**
```
Build 7490:
- FAILED: 10 tests (code issues)
- SKIPPED: 5 tests (env issues)

Build 7500:
- FAILED: 8 tests (code issues) â¬‡ï¸ IMPROVING
- SKIPPED: 10 tests (env issues) â¬†ï¸ DEGRADING

Code Health: IMPROVING (10 â†’ 8 fails)
Environment Health: DEGRADING (5 â†’ 10 skips)

Action: Code team is doing well, infrastructure team needs attention
```
âœ… Clear distinction between code and environment trends

---

### Use Case 4: Pre-Release Decision

**Scenario:** Decide if build is ready for production

**Before:**
```
staging_build_7500: 13 failures (92.5% pass rate)
Decision: NOT READY (too many failures)
```
âŒ Might reject good build

**After:**
```
staging_build_7500:
- FAILED: 1 test (minor UI issue)
- SKIPPED: 3 tests (staging env only, not in prod)

Code Quality: 99.4% (170/171 passed)
Environment: 3 staging-specific devices unavailable

Decision: READY FOR PROD
Reason: Only 1 minor fail, skips are staging-specific
```
âœ… Better decision with context

---

## ğŸ”§ Implementation Details

### Data Flow

```
test_results_analyzer_full_error.py
  â†“ (extracts FAILED and SKIPPED separately)
analyzer_wrapper.py
  â†“ (passes skipped_tests count)
report_analyzer_web.py
  â†“ (displays 5 metrics: Total, Passed, Failed, Skipped, Mode)
AI Analysis
  â†“ (analyzes FAILED and SKIPPED in separate sections)
Display Results
```

### Expected Analyzer Output

The `test_results_analyzer_full_error.py` should now return:
```python
{
    'total_tests': 174,
    'failed_tests': 10,       # Actual failures
    'skipped_tests': 3,        # Skipped tests (NEW)
    'failures': [
        {'test_name': 'test1', 'status': 'FAILED', ...},
        {'test_name': 'test2', 'status': 'SKIPPED', ...},  # Status distinguishes
        ...
    ]
}
```

---

## ğŸ¨ Visual Comparison

### Before: Everything Mixed

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test Results                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total: 174  Passed: 161  Failed: 13    â”‚
â”‚ Pass Rate: 92.5%                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FAILURES:                              â”‚
â”‚ 1. test_config_drift (FAILED)          â”‚
â”‚ 2. test_device_check (SKIPPED)         â”‚
â”‚ 3. test_cloud_api (SKIPPED)            â”‚
â”‚ 4. test_validation (FAILED)            â”‚
â”‚ ...                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
âŒ Hard to distinguish types

### After: Clear Separation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test Results                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total: 174  âœ… Passed: 161 (94.2%)  âŒ Failed: 10 (5.8%)   â”‚
â”‚ â­ï¸ Skipped: 3 (1.7%)                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FAILED TESTS (Code Issues):                                â”‚
â”‚ 1. test_config_drift - Config state timeout               â”‚
â”‚ 2. test_validation - Assertion error                      â”‚
â”‚ ...10 total                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SKIPPED TESTS (Environment Issues):                        â”‚
â”‚ 1. test_device_check - Device c220m5 unavailable          â”‚
â”‚ 2. test_cloud_api - Cloud API unreachable                 â”‚
â”‚ 3. test_prerequisites - Missing dependency                â”‚
â”‚                                                            â”‚
â”‚ Pattern: All skips are infrastructure-related              â”‚
â”‚ Action: Check device health and cloud connectivity        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
âœ… Crystal clear separation

---

## ğŸš€ Benefits Summary

### For QA Engineers
1. **Faster Triage** - Immediately see if it's code or environment
2. **Better Priority** - Fix code issues first, environment issues second
3. **Device Tracking** - Identify which devices are problematic
4. **Cloud Health** - Monitor cloud infrastructure stability

### For Dev Teams
1. **Accurate Code Quality** - Pass rate excludes environment skips
2. **Focus on Real Bugs** - Don't waste time on environment issues
3. **Better Metrics** - Code quality not penalized for bad infrastructure

### For DevOps/Infrastructure Teams
1. **Environment Visibility** - See which devices/services are failing
2. **Trend Tracking** - Monitor if environment is degrading over time
3. **Targeted Fixes** - Know exactly which infrastructure to fix

### For Managers
1. **Better Decisions** - Don't block releases due to environment issues
2. **Team Accountability** - Clear separation between dev and ops issues
3. **Resource Allocation** - Know where to focus efforts (code vs. infra)

---

## ğŸ“ Testing Checklist

After implementing these changes, verify:

- [ ] UI shows 5 columns: Total, Passed, Failed, Skipped, Mode
- [ ] FAILED and SKIPPED have separate percentages
- [ ] Pass rate excludes skipped tests from denominator
- [ ] AI analysis has separate sections for FAILED and SKIPPED
- [ ] Comparison table shows FAILED and SKIPPED separately
- [ ] History displays skip count if > 0
- [ ] Comparison prompt analyzes environment vs. code health

---

## ğŸ”„ Migration Notes

### If Analyzer Doesn't Return skipped_tests Yet

The code handles this gracefully:
```python
skipped_tests = analyzer_results.get('skipped_tests', 0)  # Defaults to 0
```

**Until analyzer is updated:**
- `skipped_tests` will be 0
- UI will show: "â­ï¸ Skipped: 0 (0%)"
- Calculations still work correctly
- No errors or crashes

**After analyzer is updated:**
- `skipped_tests` will have actual count
- Separation will be meaningful
- Full benefits realized

---

## ğŸ“š Related Files

Files modified for this feature:
1. `circuit_langchain_summarizer.py` - AI prompt updates
2. `report_analyzer_web.py` - UI metrics and comparison table
3. `utils/session_manager.py` - History display and comparison data

Files that should be updated (by user):
1. `test_results_analyzer_full_error.py` - Add SKIP as separate status

---

## ğŸ‰ Summary

**FAIL vs. SKIP separation provides:**

âœ… **Clarity** - Know if issue is code or environment  
âœ… **Speed** - Faster triage and diagnosis  
âœ… **Accuracy** - Better code quality metrics  
âœ… **Insights** - Identify device/cloud problems  
âœ… **Decisions** - Make better release decisions  

**This is a game-changer for QA efficiency!** ğŸš€

